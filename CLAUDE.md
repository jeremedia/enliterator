# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Enliterator Build Assistant v2.1 (Blank-Slate, Rails 8, OpenAI Responses API + Fine-Tune)

> **Purpose**: Stand up **Enliterator** from scratch as a Rails 8 app that turns a dropped data collection into an **enliterated dataset** and a **literate runtime** (dialogue + deliverables). This file is the **single source of truth** for Claude Code in this repo.
>
> **Zero-history rule**: Ignore any prior prototypes or contracts. Implement only what is written here.

---

## 0) Read-first requirements

- Read `/docs/enliterator_enliterated_dataset_literate_runtime_spec_v_1.md` (the spec). Extract:
  - **Ten Pool Canon** + Optional Domain Pools.
  - **Relation Verb Glossary** (closed set; forward ↔ reverse).
  - **Zero-touch pipeline** stages, acceptance gates, evaluation rubric.
  - **Dialogue & Delivery** adapter contract and endnote requirements.
- Fail early if the spec is missing; do not scaffold until present.

---

## 1) System role

You are **Claude Code — Enliterator Build Assistant**. Build a Rails 8 monolith that:

1. Runs the **zero-touch enliteration pipeline** on a single dropped bundle.
2. Materializes the **knowledge graph** (Neo4j) and **retrieval indices** (pgvector).
3. Exposes an **MCP server** with tools for extraction, search, bridging, fetching, spatial neighbors, and persona style.
4. Provides a **literate runtime** that answers, cites, and renders artifacts via adapters (webpage/markdown/pdf/table/map/timeline/outline/voice).
5. Enforces **Provenance & Rights** everywhere.
6. Uses the **OpenAI Responses API** via the official **openai** Ruby gem, including **Structured Outputs** for entity extraction.
7.  **Trains a lightweight fine-tuned model** from the knowledge graph to provide the **base-level literate interface** (canon/verbs/pathing) for the dataset.

Musts: Rails 8 conventions; Postgres (ops store), Neo4j (graph), Redis, **Solid Queue/Cache**, pgvector; deterministic tests; no guessed rights.

---

## 2) Architecture

**Services**: Postgres, Redis, Neo4j, (optional) S3/MinIO for blobs, pgvector.

**Rails areas** (modules/services):

- `Ingest/` — bundle discovery, hashing, MIME routing, OCR if needed.
- `Rights/` — rights & provenance ledger; derives `publishability`, `training_eligibility`.
- `Lexicon/` — canonical terms, surface/negative forms; normalization.
- `Pools/` — Ten Pool Canon models; serializers to graph.
- `Graph/` — Cypher writers/readers; constraints; **path textization**.
- `Embedding/` — corpora builders (repr + canonical + path); pgvector indices.
- `Runtime/` — Q&A orchestrators; gap/coverage ledger; persona narrative; delivery adapters.
- `MCP/` — tool server with strict I/O validation and audits.
- `Models/` — **Fine-tune dataset builders** and routing (see §9).

---

## 3) Zero-touch pipeline (implement exactly)

1. **Intake** → discover, hash, dedupe, partition by media.
2. **Rights & provenance** → attach license/consent; derive publishability/training eligibility; quarantine ambiguous.
3. **Lexicon bootstrap** → canonical terms, surface/negative forms, canonical descriptions.
4. **Pool filling** → extract Ten Pools + edges using the verb allow-list; assign ids, time fields; record path provenance; populate Spatial when present.
5. **Graph assembly** → load nodes/edges; enforce constraints; resolve dupes.
6. **Representations & retrieval** → build `repr_text` and **path sentences**; index in pgvector (exclude non-eligible text).
7. **Literacy scoring & gaps** → coverage, flow density, temporal completeness, spatial fidelity, rights completeness, disambiguation quality; compute maturity and Enliteracy score; emit gap report.
8. **Autogenerated deliverables** → graph ready for queries; indices; promptpacks; evaluation bundle; proposed refresh cadence.

**Acceptance gates**: as defined in the spec (id + time + rights pointer; canonical names + verbs in paths; spatial summary when used; retrieval eval passes).

---

## 4) MCP Server (contract)

**Server name**: `Enliterator MCP Server`.

### Tools

- `extract_and_link` — Extract & link entities by pool from text (uses OpenAI **Structured Outputs**).\
  **input**: `{ text, mode?: "extract|classify|link", link_threshold?: 0.6 }`\
  **output**: `{ entities: [...], ambiguities: [...], normalized_query }`

- `search` — Unified semantic + graph search with rights filtering.\
  **input**: `{ query, top_k?: 10..25, pools?: [...], date_from?, date_to?, require_rights?: "public|internal|any"=public, diversify_by_pool?: true, include_trace?: true }`\
  **output**: `{ items: [...], meta: { total_estimate, pool_counts } }`

- `fetch` — Retrieve full record + relations/timeline.\
  **input**: `{ id, include_relations?: true, relation_depth?: 1..3, pools?, as_of? }`\
  **output**: `{ id, title, body, fields, pools, relations, timeline, provenance, rights, versions }`

- `bridge` — Find items that connect concepts/pools with explicit **path**.\
  **input**: `{ a, b, top_k?: 10 }`\
  **output**: `{ bridges: [{ id, title, pools_hit, bridge_score, path }] }`

- `location_neighbors` — Spatial neighbors, multi-year patterns, stability.\
  **input**: `{ camp_name, year?, radius: "immediate|adjacent|neighborhood" }`\
  **output**: `{ by_year: [...], stability, recurring: [...], sector_prefs: [...] }`

- `set_persona` / `clear_persona` — Persona **Style Capsule** management with rights summary.

- **Helpers**: `explain_path(ids[])`, `rights_check(ids[], intended_use)`.

**Global rules**: include `rights` & `provenance`; return readable `trace`/`path`; treat unknown pools as no-op filters.

---

## 5) OpenAI Responses API (Ruby) — canonical usage

**CRITICAL**: Use the **official `openai` Ruby gem v0.16.0+** and the **Responses API** with **Structured Outputs**. This is REQUIRED for MCP server functionality. The Responses API ensures reliable, schema-compliant JSON outputs for entity extraction.

**Gem & client**

```ruby
# Gemfile
gem "openai", "~> 0.16.0"  # MUST use official gem v0.16.0 or higher
```

```ruby
# config/initializers/openai.rb
OPENAI = OpenAI::Client.new(
  api_key: ENV.fetch("OPENAI_API_KEY"),
  timeout: 120,
  max_retries: 2
)
```

**Structured Outputs for `extract_and_link`** — MUST use response_format with json_schema:

```ruby
response = OPENAI.chat.completions.create(
  messages: messages,
  model: "gpt-4o-2024-08-06",  # or gpt-4o-mini-2024-07-18
  response_format: {
    type: "json_schema",
    json_schema: {
      name: "EntityExtraction",
      strict: true,  # REQUIRED for guaranteed schema compliance
      schema: {
        type: "object",
        properties: {
          entities: { type: "array", items: entity_schema },
          ambiguities: { type: "array", items: ambiguity_schema },
          normalized_query: { type: "string" }
        },
        required: ["entities", "ambiguities", "normalized_query"],
        additionalProperties: false
      }
    }
  },
  temperature: 0  # MUST use 0 for deterministic extraction
)
```

**Key Requirements**:
- MUST use models that support Structured Outputs (gpt-4o-2024-08-06 or newer)
- MUST set `strict: true` in json_schema for guaranteed compliance
- MUST define complete JSON schemas for all MCP tool responses
- Cache schema definitions for performance
- Temperature MUST be 0 for extraction tasks

**Responses for Q&A drafting** — use a lightweight model for drafting answers; ground with MCP fetches and include citations in adapters.

---

## 6) Retrieval indices (pgvector)

- **Entity index** from `repr_text` + canonical descriptions; store `pool`, `entity_id`, rights flags.
- **Path index** from textized paths; store `path_hash`, participating ids, rights flags.
- Filter at query time by `require_rights`.

---

## 7) Knowledge graph (Neo4j)

- Node labels: `Idea`, `Manifest`, `Experience`, `Relational`, `Evolutionary`, `Practical`, `Emanation`, `Rights`, `Lexicon`, `Intent` (+ optional `Actor`, `Spatial`, `Evidence`, `Risk`, `Method`).
- Relationship types: allow-list = Relation Verb Glossary.
- Constraints: unique `id`; mandatory time field; rights pointer on every node.
- Path textization: deterministic sentence builder using canonical Idea names + verbs.

---

## 8) Dialogue & Delivery

**Dialogue**: maintain session state (intent, constraints, persona, grounded entities, gaps). Show a **Path** and cite sources; spatial claims trigger a spatial endnote.

**Delivery adapters**: service objects with lifecycle: Inputs → Preflight (rights/grounding/coverage) → Render → Validate → Return. Implement: `make_webpage`, `export_markdown`, `export_pdf`, `export_table`, `make_map`, `make_timeline`, `make_outline`, `voice_script`.

Outputs must include citations, rights echo, and a concise path summary; spatial endnote when relevant.

---

## 9) Model fine-tune from the knowledge graph (base-level literate interface)

**Why**

- The base LLM understands English but not your **canon** (pools, verbs, canonical names). A small fine-tune teaches: canonical naming, verb usage, path narration, query normalization, and guardrails. This becomes the **always-on, rights-aware interface** that routes to search/fetch for facts.

**Objectives** (model should reliably):

1. Map free-form phrases to **canonical terms** and pools (Lexicon normalization).
2. Produce **path sentences** from graph snippets using the Relation Verb Glossary.
3. Suggest **next hops** (e.g., which MCP tool to call and why) given an intent.
4. Rewrite prompts into **normalized queries** that diversify by pool and respect date/space bounds.
5. Detect when the answer requires **RAG** (graph/fetch) vs. can be answered from canon.
6. Enforce **rights-first phrasing** (avoid quoting when `publishability=false`).

**Training dataset assembly** (automatic jobs):

- **Canonical term pairs**: `(user_phrase, canonical_term + pool)` from Lexicon (`surface_forms`, `negative_surface_forms`).
- **Path narrations**: random walks across 2–5 hops; input = edge list; output = path sentence.
- **Tool routing cases**: intent + constraints → `{tool, params}` (e.g., spatial questions → `location_neighbors`).
- **Normalization rewrites**: raw user queries → normalized queries (time/space scoping, pool diversification).
- **Rights guardrails**: pairs showing restricted vs. public Experience phrasing.

**Data sources**

- Neo4j graph (nodes/edges), Lexicon tables, Coverage ledger, Rights ledger.
- Only include text with `training_eligibility=true`.

**Schema for fine-tune examples** (store in Postgres + emit JSONL):

```json
{ "task": "canon_map", "input": "temple of tears", "output": {"canonical": "Temple", "pool": "manifest"} }
{ "task": "path_text", "input": {"nodes": ["idea:radical_inclusion","manifest:camp_x","experience:story_1"], "edges": ["embodies","elicits"]}, "output": "Idea(Radical Inclusion) → embodies → Manifest(Camp X) → elicits → Experience(Story #1)." }
{ "task": "route", "input": {"intent": "who were our neighbors in 2019?"}, "output": {"tool": "location_neighbors", "params": {"camp_name": "...", "year": 2019, "radius": "adjacent"}} }
{ "task": "normalize", "input": "show me camps near 3:30 portal last few years", "output": "camps near 3:30 Portal, 2018–2024, diversify by pool" }
{ "task": "rights_style", "input": "quote testimonial X", "output": "paraphrase Experience; rights restricted" }
```

**Procedure**

1. Build `FineTune::DatasetBuilder` service that queries the graph and lexicon to emit stratified JSONL shards per task.
2. Split into train/val/test; keep per-task distributions.
3. Use the OpenAI Ruby gem to **create a fine-tune** on a supported lightweight model (e.g., a *mini* family) for low latency and cost. Keep base models configurable.
4. Track run ids, metrics, and the deployed model name in `runtime.models` table.
5. Add a **router**: for every user turn, first call the **fine-tuned model** to (a) normalize query, (b) decide tool plan, (c) draft a path scaffold. Then execute MCP calls and assemble the final answer.
6. Add evaluations: exact-match for `canon_map`, BLEU/ROUGE for `path_text`, tool-plan accuracy for `route`, and rights phrasing compliance.

**Ruby sketch** (non-binding):

```ruby
# app/services/fine_tune/trainer.rb
module FineTune
  class Trainer
    def self.create_job(jsonl_path)
      client = OPENAI
      client.fine_tuning.jobs.create(
        training_file: Uploads.upload(jsonl_path),
        model: ENV.fetch("OPENAI_FT_BASE", "gpt-mini"), # configurable
        suffix: "enliterator-canon-v1"
      )
    end
  end
end
```

**Routing at runtime**

- If the fine-tune returns `route: location_neighbors` → call that tool.
- If it returns `route: search` with pool filters → include them.
- Always pass the **normalized_query** to MCP; keep the LLM answerer separate and grounded by fetch results.

**Failure modes**

- **Drift**: re-train when graph or lexicon versions change significantly; keep version pins.
- **Overreach**: if the model starts answering from recall, clamp behavior: "Plan only; facts via MCP."
- **Latency**: cache router outputs for repeated intents; keep the base model small.

---

## 10) Persona Style Layer (optional)

Implement `set_persona/clear_persona` returning a **Style Capsule** (tone, cadence, vocabulary, devices, era, confidence, sources, rights summary). Style shapes phrasing only; facts come from fetched records. Add Solid Cache for capsules and Solid Queue jobs to build/refresh.

---

## 11) Testing & QA

- **Specs**: pipeline stages; extraction (schema conformance); graph writer; retrieval; MCP tools; adapters; persona layer; rights gating; **fine-tune dataset builder**; router accuracy.
- **Eval**: groundedness %, rights clean, coverage fit, adapter fidelity, retrieval quality; tool-plan accuracy; canon mapping accuracy; path text BLEU.
- **Red-team**: ambiguous names, missing rights, conflicting placements, sparse persona corpus, misleading routes.
- **Performance**: `search` p95 ≤ 800ms (`top_k ≤ 10`); router (fine-tuned model) p95 ≤ 150ms.

---

## 12) Environment & Ops

- ENV: `OPENAI_API_KEY`, `OPENAI_MODEL`, `OPENAI_MODEL_ANSWER`, `OPENAI_FT_BASE`, `OPENAI_FT_MODEL`, `DATABASE_URL`, `REDIS_URL`, `NEO4J_URL`, `VECTOR_DB_URL`, `STORAGE_BUCKET`.
- Scripts: `bin/setup`, `bin/dev` (web, worker), `bin/ci`.
- Rake: `enliterator:ingest`, `enliterator:graph:sync`, `enliterator:embed:refresh`, `enliterator:fine_tune:build`, `enliterator:evaluate`.
- Docker compose for PG/Redis/Neo4j; healthchecks; seeded demo.

---

## 13) Common Development Commands

### Initial Rails Setup
```bash
# Create new Rails 8 app
rails new enliterator --database=postgresql --skip-test --css=tailwind

# Install dependencies
bundle add openai neo4j neo4j-ruby-driver neighbor solid_queue solid_cache

# Setup databases
rails db:create
rails db:migrate
```

### Development
```bash
# Start Rails server with Solid Queue worker
bin/dev

# Run tests
bin/rails test
bin/rails test:system

# Run linters
bundle exec rubocop
bundle exec erblint --lint-all

# Run specific service tests
bin/rails test test/services/ingest/**/*_test.rb
```

### Pipeline Operations
```bash
# Ingest a data bundle
bin/rails enliterator:ingest[path/to/bundle.zip]

# Sync graph database
bin/rails enliterator:graph:sync

# Refresh embeddings
bin/rails enliterator:embed:refresh

# Build fine-tune dataset
bin/rails enliterator:fine_tune:build

# Run evaluation suite
bin/rails enliterator:evaluate
```

---

## 14) Coding conventions

- Normalize to **canonical names and casing**. Store synonyms/negatives in Lexicon.
- Never surface specifics (roles, placements) without a fetched source.
- Always attach rights in tool outputs; filter Experience without public consent when `require_rights=public`.
- Spatial adjacency must use `location_neighbors` (not semantic guesses).
- The **fine-tuned model routes and normalizes**, it does **not** invent facts.

---

## 15) Definition reminders (quote in code comments)

- **Literate technology**: software that converses in natural language, shows its reasoning paths and sources, adapts to user intent and constraints, and produces deliverables—treating data as a partner in meaning rather than a passive store.
- **Enliteracy**: the process that makes a dataset literate by modeling it into pools of meaning and explicit flows between them—plus rights, provenance, and a canonical lexicon—so the system can answer *why*, *how*, and *what's next*, not only *what*.